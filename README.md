## ğŸ” í”„ë¡œì íŠ¸ ì†Œê°œ

ë§ë¶„ë¦¬(Network Segregation)ëŠ” ê¸°ì—…ê³¼ ì •ë¶€ê¸°ê´€ì—ì„œ ë‚´ë¶€ë§ê³¼ ì¸í„°ë„·ë§ì„ ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶„ë¦¬í•´ ì‚¬ì´ë²„ ë³´ì•ˆì„ ê°•í™”í•˜ëŠ” ì¤‘ìš”í•œ ì „ëµì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ë§ë¶„ë¦¬ í™˜ê²½ì—ì„œë„ ë‚´ë¶€ìì˜ ì˜ë„ì ì¸ ë³´ì•ˆ ìœ„ë°˜ì´ë‚˜ ì‚¬ìš©ì ì‹¤ìˆ˜ë¡œ ì¸í•œ ë„¤íŠ¸ì›Œí¬ í˜¼ìš©ì´ ë¹ˆë²ˆíˆ ë°œìƒí•˜ë©°, ê¸°ì¡´ì˜ ë°©í™”ë²½(Firewall), ì¹¨ì…íƒì§€ì‹œìŠ¤í…œ(IDS), ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼ì œì–´(NAC)ì™€ ê°™ì€ ë³´ì•ˆ ì†”ë£¨ì…˜ë§Œìœ¼ë¡œëŠ” ì´ëŸ° ìœ„í˜‘ì„ ì™„ë²½íˆ ì°¨ë‹¨í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë§ë¶„ë¦¬ í™˜ê²½ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ë‚´ë¶€ ë³´ì•ˆ ìœ„í˜‘ì„ ì¡°ê¸°ì— íƒì§€í•˜ê³  ëŒ€ì‘í•˜ê¸° ìœ„í•œ **ì‹¤ì‹œê°„ ì´ìƒ íŠ¸ë˜í”½ íƒì§€ ë° ì°¨ë‹¨ ì‹œìŠ¤í…œ**ì„ ì œì•ˆí•©ë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” íŠ¹íˆ **Variational Autoencoder(VAE)**, **Anomaly Transformer**, **ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ(Naive Bayes)** ê¸°ë²•ì„ ê²°í•©í•˜ì—¬ ë¹„ì •ìƒ íŠ¸ë˜í”½ì„ ì •í™•íˆ íƒì§€í•˜ê³ , ë‹¨ë§ì˜ ì´ìƒí–‰ìœ„ë¥¼ í™•ë¥ ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ì•™ìƒë¸” ê¸°ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

ë³¸ ì‹œìŠ¤í…œì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

* SNMP, TAP, Arkime ê¸°ë°˜ì˜ ë‹¤ì°¨ì› ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ì‹¤ì‹œê°„ ìˆ˜ì§‘ ë° ë¶„ì„
* VAEë¥¼ í™œìš©í•œ ì„¸ì…˜ ë‹¨ìœ„ ì´ìƒí–‰ìœ„ íƒì§€ ë° ì°¨ì› ì¶•ì†Œ(latent vector) ë¶„ì„
* Anomaly Transformerë¥¼ ì´ìš©í•œ ì‹œê³„ì—´ ê¸°ë°˜ ì„¸ì…˜ íë¦„ ì´ìƒí–‰ìœ„ íƒì§€
* ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ê¸°ë²•ì„ ì´ìš©í•œ ë‹¨ë§ë³„ ì´ìƒí–‰ìœ„ í™•ë¥ ì  í‰ê°€ ë° ìš°ì„ ìˆœìœ„ ì§€ì •

ë³¸ ì•™ìƒë¸” ì ‘ê·¼ë²•ì„ í†µí•´ ë‚´ë¶€ì ìœ„í˜‘, ë„¤íŠ¸ì›Œí¬ í˜¼ìš©, ì•”í˜¸í™”ëœ íŠ¸ë˜í”½ ë‚´ ì´ìƒì§•í›„ ë“±ì„ ê¸°ì¡´ ë°©ì‹ë³´ë‹¤ í›¨ì”¬ ì •ë°€í•˜ê²Œ íƒì§€í•˜ë©°, ê¶ê·¹ì ìœ¼ë¡œ ë¬¼ë¦¬ì  ë§ë¶„ë¦¬ í™˜ê²½ì—ì„œ ê°•ë ¥í•œ **ì‹¤ì‹œê°„ ë³´ì•ˆ ê´€ì œ ì¸í”„ë¼ êµ¬ì¶•**ì— ê¸°ì—¬í•˜ê³ ì í•©ë‹ˆë‹¤.


![image](https://github.com/user-attachments/assets/e59eee47-85a9-4ae6-bb57-fc14b4a7ee6d)

## ğŸŒ ë§ë¶„ë¦¬(Network Segregation) í™˜ê²½ì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì„±ë„

ë³¸ ê·¸ë¦¼ì€ ë§ë¶„ë¦¬ í™˜ê²½ì—ì„œ ê¸°ê´€ ë° ê¸°ì—…ì´ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ êµ¬ì„±ì„ ë‚˜íƒ€ë‚´ê³  ìˆìŠµë‹ˆë‹¤. ë§ë¶„ë¦¬ í™˜ê²½ì€ ì£¼ë¡œ ì•„ë˜ì™€ ê°™ì´ ì„¸ ê°€ì§€ ì˜ì—­ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

* **ì¸í„°ë„· ë„¤íŠ¸ì›Œí¬(Internet Network)**: ì™¸ë¶€ ì¸í„°ë„·ê³¼ ì§ì ‘ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ë³´ì•ˆ ì¥ì¹˜ë¥¼ í†µí•´ ì™¸ë¶€ ìœ„í˜‘ì„ íƒì§€í•˜ê³  ì°¨ë‹¨í•©ë‹ˆë‹¤. ë˜í•œ ì›¹ ì„œë¹„ìŠ¤ë¥¼ ì•ˆì „í•˜ê²Œ ì œê³µí•˜ê¸° ìœ„í•´ DMZ(Demilitarized Zone) ì˜ì—­ì„ ìš´ì˜í•©ë‹ˆë‹¤.
* **ì‚¬ì„¤ ë„¤íŠ¸ì›Œí¬(Private Network)**: ë‚´ë¶€ ì—…ë¬´ë§ìœ¼ë¡œ, ì¸í„°ë„·ë§ê³¼ ì™„ì „íˆ ê²©ë¦¬í•˜ì—¬ ê¸°ë°€ì„±ì„ ìœ ì§€í•˜ê³  ë°ì´í„°ì˜ ë¬´ê²°ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ë‚´ë¶€ ì„œë¹„ìŠ¤ ë° ë³´ì•ˆ ì‹œìŠ¤í…œì´ ìœ„ì¹˜í•©ë‹ˆë‹¤.
* **ì‚¬ìš©ì ì˜ì—­(User Area)**: ì‹¤ì œ ì‚¬ìš©ìë“¤ì´ ì—…ë¬´ë¥¼ ìˆ˜í–‰í•˜ëŠ” êµ¬ì—­ìœ¼ë¡œ, ì¸í„°ë„·ë§ê³¼ ë‚´ë¶€ ì—…ë¬´ë§ì´ ë™ì‹œì— ì œê³µë˜ì–´ ì‚¬ìš©ì í¸ì˜ì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.

ë§ë¶„ë¦¬ í™˜ê²½ì˜ ë„ì…ì€ ë³´ì•ˆì„± ê°•í™”ì™€ ë°ì´í„° ë³´í˜¸ë¼ëŠ” ì¸¡ë©´ì—ì„œ ëª…í™•í•œ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê´€ë¦¬ì ì¸ ê´€ì ì—ì„œ ë³¼ ë•ŒëŠ” ê´€ë¦¬í•´ì•¼ í•  ë„¤íŠ¸ì›Œí¬ í¬ì¸íŠ¸ê°€ ì¦ê°€í•˜ì—¬ ê´€ë¦¬ ë³µì¡ì„± ë° ìš´ì˜ ë¹„ìš©ì´ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

íŠ¹íˆ ì‚¬ìš©ì ì˜ì—­ì—ì„œëŠ” ì‚¬ìš©ìê°€ ì •í™•íˆ ì¸ì§€í•˜ì§€ ëª»í•˜ê³  ì—…ë¬´ë§ê³¼ ì¸í„°ë„·ë§ì„ í˜¼ìš©í•  ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©°, ì´ëŠ” ë‚´ë¶€ ë°ì´í„°ê°€ ì™¸ë¶€ë¡œ ë…¸ì¶œë˜ëŠ” ì‹¬ê°í•œ ë³´ì•ˆ ìœ„í˜‘ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ê´€ë¦¬ìì˜ ì‹¤ìˆ˜ë‚˜ ì˜ëª»ëœ ë„¤íŠ¸ì›Œí¬ ì„¤ì •ìœ¼ë¡œ ì¸í•´ ë„¤íŠ¸ì›Œí¬ í˜¼ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆì–´ ë³´ì•ˆìƒ ë¬¸ì œê°€ ì§€ì†ì ìœ¼ë¡œ ë°œìƒí•©ë‹ˆë‹¤.



## ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ ë°©ë²•

ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë§ë¶„ë¦¬ í™˜ê²½ì—ì„œ ì •í™•í•˜ê³  ì‹ ë¢°ì„± ë†’ì€ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ ë°ì´í„°ì˜ ì •í™•ì„±ê³¼ ë¬´ê²°ì„±ì„ í™•ë³´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì²˜ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.

![image](https://github.com/user-attachments/assets/8b75728e-db3f-42b2-9277-d45c483ced95)

### ğŸ”¹ ë°ì´í„° ìˆ˜ì§‘ í™˜ê²½ êµ¬ì¶•

* **ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ì‹¤ì‹œê°„ ìº¡ì²˜**

  * **Arkimeì™€ Wireshark** ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ê³¼ ì„¸ì…˜ ë°ì´í„°ë¥¼ ìº¡ì²˜Â·ë¶„ì„í•©ë‹ˆë‹¤.
  * ë„¤íŠ¸ì›Œí¬ì˜ ë¯¸ëŸ¬ë§ í¬íŠ¸ì™€ TAP(Test Access Point)ë¥¼ í†µí•´ ëª¨ë“  VLANì˜ íŠ¸ë˜í”½ì„ ìˆ˜ì§‘í•˜ê³ , ì´ë¥¼ ë¶„ì„ ì„œë²„ë¡œ ì§‘ì•½í•˜ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤.

* **ìŠ¤ìœ„ì¹˜ í¬íŠ¸ í†µê³„ ì •ë³´ ìˆ˜ì§‘**

  * SNMP(Simple Network Management Protocol)ë¥¼ ì´ìš©í•˜ì—¬ ë¸Œë¡œë“œìºìŠ¤íŠ¸, ë©€í‹°ìºìŠ¤íŠ¸, ìœ ë‹ˆìºìŠ¤íŠ¸, ì˜¤ë¥˜ íŒ¨í‚· ë“± ë‹¤ì–‘í•œ í¬íŠ¸ í†µê³„ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ë¶„ì„ì— í™œìš©í•©ë‹ˆë‹¤.

### ğŸ”¹ ë°ì´í„° ì •í™•ì„± ë° ë¬´ê²°ì„± ë³´ì¥ ë°©ë²•

* **ë‹¤ì¤‘ ë„¤íŠ¸ì›Œí¬ í˜¼ìš© íƒì§€**

  * ì—¬ëŸ¬ ë„¤íŠ¸ì›Œí¬ê°€ ì‚¬ìš©ì ì‹¤ìˆ˜ ë˜ëŠ” ì˜ë„ì ì¸ í˜¼ìš©ìœ¼ë¡œ ì¸í•´ ì—°ê²°ë  ê²½ìš° ì‹¬ê°í•œ ë³´ì•ˆ ìœ„í˜‘ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ìŠ¤ìœ„ì¹˜ ì¥ë¹„ì˜ SNMP ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ í˜¼ìš© ìƒíƒœë¥¼ íƒì§€í•©ë‹ˆë‹¤.
  * IPì™€ MAC ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ë¹„ì •ìƒì  ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœë¥¼ ì¡°ê¸°ì— ë°œê²¬í•˜ê³  ê´€ë¦¬ìì—ê²Œ ê²½ê³ í•©ë‹ˆë‹¤.

* **ë¹„ì¸ê°€ ë„¤íŠ¸ì›Œí¬ ì¥ë¹„ íƒì§€ ë° ì œê±°**

  * ë¹„ì¸ê°€ ì¥ë¹„(NAT ë¼ìš°í„° ë“±)ê°€ ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ì— ë¬´ë‹¨ìœ¼ë¡œ ì—°ê²°ë˜ë©´ ë³´ì•ˆ ì •ì±…ì„ ìš°íšŒí•˜ì—¬ ì¹¨ì…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. SNMP ë°ì´í„°ì™€ TTL(Time to Live), ARP ì •ë³´ ë“± ë„¤íŠ¸ì›Œí¬ í”„ë¡œí† ì½œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë¹„ì¸ê°€ ì¥ë¹„ë¥¼ íƒì§€í•˜ê³  ì œê±°í•©ë‹ˆë‹¤.
  * ì´ë¥¼ í†µí•´ ë¹„ì •ìƒ ë°ì´í„° ë°œìƒì„ ìµœì†Œí™”í•˜ê³  ë°ì´í„°ì˜ ë¬´ê²°ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.

## ğŸ—‚ï¸ ìˆ˜ì§‘ ë°ì´í„° ì¢…ë¥˜ ë° íŠ¹ì§•

ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë§ë¶„ë¦¬ í™˜ê²½ì˜ ì´ìƒ í–‰ìœ„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ íƒì§€í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ë“¤ì„ ìˆ˜ì§‘í•˜ì—¬ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ê° ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ í™œìš© ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

### ğŸ“Œ 1. íŒ¨í‚· ë°ì´í„° ë° ì„¸ì…˜ ì •ë³´

* **ìˆ˜ì§‘ ë°©ë²•**

  * ì˜¤í”ˆì†ŒìŠ¤ íˆ´ì¸ **Arkime** ë° **Wireshark**ë¥¼ í™œìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ê³¼ ì„¸ì…˜ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘Â·ì €ì¥í–ˆìŠµë‹ˆë‹¤.
  * ëª¨ë“  íŠ¸ë˜í”½ì€ ì„¸ì…˜ ë‹¨ìœ„ë¡œ êµ¬ì„±í•˜ì—¬ ë¶„ì„ì´ ìš©ì´í•˜ë„ë¡ ë©”íƒ€ë°ì´í„°ë¡œ ìƒ‰ì¸í™”í–ˆìŠµë‹ˆë‹¤.

* **ì£¼ìš” ìˆ˜ì§‘ í•­ëª©**

  * íŒ¨í‚· ë°œìƒ ì‹œê°(timestamp)
  * ì„¸ì…˜ ê¸¸ì´(length), ì§€ì† ì‹œê°„(duration)
  * TCP/IP í”„ë¡œí† ì½œ ì„¸ë¶€ í”Œë˜ê·¸(SYN, ACK ë“±) ìˆ˜ì¹˜
  * í´ë¼ì´ì–¸íŠ¸Â·ì„œë²„ ê°„ ë°ì´í„° ì „ì†¡ëŸ‰ ë° íŒ¨í‚· ìˆ˜ ë“± **ì´ 82ê°€ì§€ ì´ìƒì˜ ì„¸ì…˜ í†µê³„ ì§€í‘œ**

* **í™œìš© ë°©ì•ˆ**

  * ìˆ˜ì§‘ëœ ì„¸ì…˜ ë°ì´í„°ëŠ” **Variational Autoencoder(VAE)** ëª¨ë¸ì„ ì´ìš©í•´ ì´ìƒ íŠ¸ë˜í”½ íƒì§€ í•™ìŠµì— í™œìš©í•©ë‹ˆë‹¤.
  * ì´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì‹œê°„ ì„¸ì…˜ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ ë¹„ì •ìƒ í–‰ìœ„ë¥¼ ë¹ ë¥´ê²Œ ì‹ë³„í•˜ê³  ëŒ€ì‘í•©ë‹ˆë‹¤.

---

### ğŸ“Œ 2. ë„¤íŠ¸ì›Œí¬ ì¥ë¹„ í¬íŠ¸ë³„ í†µê³„ ì§€í‘œ ë°ì´í„°

* **ìˆ˜ì§‘ ë°©ë²•**

  * SNMP(Simple Network Management Protocol)ë¥¼ í™œìš©í•˜ì—¬ ìŠ¤ìœ„ì¹˜ í¬íŠ¸ë³„ ë‹¤ì–‘í•œ íŠ¸ë˜í”½ í†µê³„ ì§€í‘œë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.

* **ì£¼ìš” ìˆ˜ì§‘ í•­ëª©**

  * ë¸Œë¡œë“œìºìŠ¤íŠ¸(Broadcast) íŒ¨í‚· ìˆ˜
  * ë©€í‹°ìºìŠ¤íŠ¸(Multicast) íŒ¨í‚· ìˆ˜
  * ìœ ë‹ˆìºìŠ¤íŠ¸(Unicast) íŒ¨í‚· ìˆ˜
  * íê¸°(Packet Discard) ë° ì˜¤ë¥˜(Packet Error) íŒ¨í‚· ìˆ˜
  * í¬íŠ¸ë³„ ì´ íŠ¸ë˜í”½ ì–‘(IN/OUT)

* **í™œìš© ë°©ì•ˆ**

  * í¬íŠ¸ë³„ í†µê³„ ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬ í‰ìƒì‹œì˜ ì •ìƒ íŠ¸ë˜í”½ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.
  * íŠ¸ë˜í”½ ì§€í‘œì—ì„œ ì´ìƒ ì§•í›„ê°€ ë°œê²¬ë˜ë©´ ì´ë¥¼ ì¦‰ì‹œ íƒì§€í•˜ì—¬ ë¹ ë¥´ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

### ğŸ“Œ 3. í†µê³„ì  ëª¨ë¸ë§ì„ í†µí•œ í–‰ìœ„ ìˆ˜ì¹˜í™” ë° í™•ë¥ í™”

* **ìˆ˜í–‰ ë°©ë²•**

  * ê° ë‹¨ë§ì´ ì„œë²„ì— ì ‘ê·¼í•˜ëŠ” ë¹ˆë„ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í¬ì•„ì†¡ ë¶„í¬(Poisson Distribution) ê¸°ë°˜ì˜ í†µê³„ì  ëª¨ë¸ë§ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.
  * ì„œë²„ ì ‘ê·¼ í‰ê· ê°’(ëŒë‹¤, Î»)ì„ ì‚°ì¶œí•˜ì—¬ ì‹œê°„ë³„, ë‹¨ë§ë³„ ì ‘ê·¼ ë¹ˆë„ì˜ ì •ìƒÂ·ë¹„ì •ìƒì„ í†µê³„ì ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.

* **í™œìš© ë°©ì•ˆ**

  * ì´ìƒ í–‰ìœ„ë¥¼ ë³´ì´ëŠ” ë‹¨ë§ì˜ ì ‘ê·¼ ë¹ˆë„ë¥¼ ìˆ˜ì¹˜í™”í•˜ì—¬ ëª…í™•í•˜ê²Œ íƒì§€í•©ë‹ˆë‹¤.
  * AI ê¸°ë°˜ ëª¨ë¸ì„ ë³´ì™„í•˜ëŠ” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ íƒì§€ ë°©ë²•ìœ¼ë¡œ í™œìš©ë©ë‹ˆë‹¤.

---

# ğŸ” ë§ë¶„ë¦¬ í™˜ê²½ ì´ìƒ íŠ¸ë˜í”½ íƒì§€ ì‹œìŠ¤í…œ

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë§ë¶„ë¦¬(Network Segregation) í™˜ê²½ì—ì„œ ë°œìƒ ê°€ëŠ¥í•œ ì´ìƒ íŠ¸ë˜í”½ ë° ë‚´ë¶€ ë³´ì•ˆ ìœ„í˜‘ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ íƒì§€í•˜ê³  ì°¨ë‹¨í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.  
Variational Autoencoder (VAE), Anomaly Transformer, Naive Bayesë¥¼ ê²°í•©í•œ ì•™ìƒë¸” ê¸°ë°˜ì˜ íƒì§€ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ **ì•”í˜¸í™”ëœ íŠ¸ë˜í”½**, **ë„¤íŠ¸ì›Œí¬ í˜¼ìš©**, **ë¹„ì¸ê°€ ë‹¨ë§ ì ‘ì†** ë“±ì˜ ìœ„í˜‘ì— íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì‘í•©ë‹ˆë‹¤.

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ğŸ“¦project-root/
 â”£ ğŸ“‚data/                 # ì„¸ì…˜ ë° í¬íŠ¸ í†µê³„ ìˆ˜ì§‘ ë°ì´í„°
 â”£ ğŸ“‚models/               # ëª¨ë¸ ì •ì˜ (VAE, Transformer ë“±)
 â”£ ğŸ“‚notebooks/            # ë¶„ì„ìš© Jupyter ë…¸íŠ¸ë¶
 â”£ main.py                # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
 â”— README.md              # í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ
```

---

## ğŸ“Š ë°ì´í„° ì„¤ëª…

### âœ… ì„¸ì…˜ ë°ì´í„° (session_data.csv)
- ì´ 82ê°œ í•„ë“œë¡œ êµ¬ì„±
- ì£¼ìš” í•­ëª©:
  - `duration`, `tcp_flags_ack`, `client_bytes`, `server_bytes`, `packets`, `bytes_total` ë“±
- íŠ¸ë˜í”½ íë¦„ì„ ìˆ˜ì¹˜í™”í•œ ë°ì´í„° â†’ VAE í•™ìŠµì— ì‚¬ìš©

### âœ… í¬íŠ¸ í†µê³„ ë°ì´í„° (port_stats.csv)
- SNMP ê¸°ë°˜ ìˆ˜ì§‘
- ì£¼ìš” í•­ëª©:
  - `broadcast_packets`, `multicast_packets`, `packet_errors`, `port_traffic_in/out`
- ìŠ¤ìœ„ì¹˜ í¬íŠ¸ë³„ ì´ìƒ ì§•í›„ íƒì§€ì— í™œìš©

---

## ğŸ’¡  ELASTIC SERVERì—ì„œ ë°ì´í„° ê°€ì§€ê³  ì˜¤ê¸° 
```python
from elasticsearch import Elasticsearch
from datetime import datetime
import pandas as pd
import numpy as np
# Elasticsearch í˜¸ìŠ¤íŠ¸ ë° í¬íŠ¸ ì„¤ì •
es = Elasticsearch(['http://192.168.0.0:9200'])

# ì¸ë±ìŠ¤ ì´ë¦„, í˜ì´ì§€ í¬ê¸°, ì‹œê°„ ë²”ìœ„ ë° íŠ¹ì • ê°’ ì„¤ì •
index_name = 'arkime_sessions3*'
page_size = 500
start_time = datetime(2024, 1, 1)  # ì‹œì‘ ì‹œê°„ ì„¤ì •
end_time = datetime(2024, 12, 31)  # ì¢…ë£Œ ì‹œê°„ ì„¤ì •
desired_value = 'your_desired_value'  # ê°€ì ¸ì˜¤ê³ ì í•˜ëŠ” íŠ¹ì • ê°’

# Elasticsearch Scroll APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ê²€ìƒ‰
scroll_size = page_size
scroll_timeout = "1m"  # ìŠ¤í¬ë¡¤ íƒ€ì„ì•„ì›ƒ ì„¤ì •

response = es.search(
    index=index_name,
    scroll=scroll_timeout,
    size=scroll_size,
    body={
        "query": {
            "bool": {
                "must": [
                    {
                        "range": {
                            "@timestamp": {
                                "gte": start_time,
                                "lte": end_time
                            }
                        }
                    }
                ]
            }
        }
    }
)

scroll_id = response.get('_scroll_id')

hits_list = []
while True:
    # ìŠ¤í¬ë¡¤ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    scroll_response = es.scroll(scroll_id=scroll_id, scroll=scroll_timeout)
    hits = scroll_response['hits']['hits']
    if not hits:
        break  # ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ
    for hit in hits:
        hits_list.append(hit)

```

---

## ğŸ’¡ í•™ìŠµì„ ìœ„í•´ ì„¸ì…˜ ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ì‘ì—…
```python
import pandas as pd
import numpy as np
from collections import defaultdict

keys_to_extract = [
    "firstPacket", "lastPacket", "length", "ipProtocol", 'srcOuiCnt', 'dstOuiCnt',
    "tcpflags_syn", "tcpflags_syn-ack", "tcpflags_ack", "tcpflags_psh", "tcpflags_fin",
    "tcpflags_rst", "tcpflags_urg", "tcpflags_srcZero", "tcpflags_dstZero", "initRTT",
    "source_bytes", "source_packets", "destination_bytes", "destination_packets","destination_mac-cnt",
    "network_packets", "network_bytes", "client_bytes", "server_bytes", "totDataBytes",
    "segmentCnt", "http_bodyMagicCnt", "http_clientVersionCnt", "http_hostCnt",
    "http_keyCnt", "http_methodCnt", "http_pathCnt", "http_request-content-typeCnt",
    "http_request-refererCnt", "http_requestHeaderCnt", "http_response-content-typeCnt",
    "http_responseHeaderCnt", "http_serverVersionCnt", "http_statuscodeCnt", "http_uriCnt",
    "http_useragentCnt", "protocolCnt",  "http_authTypeCnt",
    "http_request-authorizationCnt", "http_userCnt", "srcDscpCnt", "ssh_hasshCnt",
    "ssh_hasshServerCnt", "ssh_versionCnt", "tls_cipherCnt", "tls_ja3Cnt",
    "tls_versionCnt",    "protocol_str_concat", 'destination_port' # Include other keys as needed
]

# í‰íƒ„í™” í•¨ìˆ˜ ì •ì˜
def flatten_json(y, parent_key='', sep='_'):
    items = {}
    for key, value in y.items():
        new_key = f"{parent_key}{sep}{key}" if parent_key else key
        if isinstance(value, dict):
            items.update(flatten_json(value, new_key, sep=sep))
        elif isinstance(value, list):
            num_sum = sum(item for item in value if isinstance(item, (int, float)))
            str_concat = ','.join(str(item) for item in value if isinstance(item, str))
            items[f'{new_key}_num_sum'] = num_sum
            items[f'{new_key}_str_concat'] = str_concat
        else:
            items[new_key] = value
    return items

# ë°ì´í„° ì¶”ì¶œ ë° í‰íƒ„í™”ë¥¼ ìœ„í•œ í•¨ìˆ˜ ì •ì˜
def extract_and_flatten(json_data):
    extracted_data = {}
    flat_data = flatten_json(json_data['_source'])
    for key in keys_to_extract:
        value = flat_data.get(key, 0)  # ê°’ì´ ì—†ì„ ê²½ìš° 0ì„ ì…ë ¥
        extracted_data[key] = value
    return extracted_data

# ë°ì´í„° ë³‘í•©ì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜
def merge_json_data(json_list):
    data_list = []
    for json_data in json_list:
        extracted_data = extract_and_flatten(json_data)
        data_list.append(extracted_data)
    return pd.DataFrame(data_list)

# ë³‘í•©í•  JSON ë°ì´í„° ë¦¬ìŠ¤íŠ¸
json_data_list = hits_list #[json_data_1,json_data_2,json_data_3,json_data_4,json_data_5,json_data_6]

# ë³‘í•© í•¨ìˆ˜ í˜¸ì¶œ
merged_df = merge_json_data(json_data_list)
merged_df = pd.concat([merged_df] * 1, ignore_index=True)

# "firstPacket"ê³¼ "lastPacket"ì˜ ì°¨ì´ê°’ ê³„ì‚° ë° ì»¬ëŸ¼ ì¶”ê°€
merged_df['packetDuration'] = merged_df['lastPacket'] - merged_df['firstPacket']

# ì›ë˜ "firstPacket"ê³¼ "lastPacket" ì»¬ëŸ¼ ì œê±°
merged_df = merged_df.drop(columns=['firstPacket', 'lastPacket'])

# 'protocol_str_concat' ì»¬ëŸ¼ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
merged_df['protocol_list'] = merged_df['protocol_str_concat'].str.split(',')
# 'protocol_str_concat' ì»¬ëŸ¼ ì‚­ì œ
merged_df.drop(columns=['protocol_str_concat'], inplace=True)

# ì§€ì •ëœ í”„ë¡œí† ì½œ ëª©ë¡
protocols = [
    "http", "https", "ftp", "sftp", "smtp", "pop3", "imap",
    "dns", "tcp", "udp", "ssh", "ssl", "telnet", "snmp", "icmp",
    "bgp", "ospf", "rip", "sip", "voip", "rdp", "mqtt", "ldap",
    "arp", "ip", "ethernet", "nfs", "smb", "quic", "ntp", "http/2", "http/3", "tls"
]

# ê° í”„ë¡œí† ì½œì„ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
for protocol in protocols:
    merged_df['pthot_'+protocol] = merged_df['protocol_list'].apply(lambda x: 1 if protocol in x else 0)

columns_to_drop = [col for col in merged_df.columns if isinstance(merged_df[col][0], (list, str))]
merged_df = merged_df.drop(columns=columns_to_drop)
```
---


## ğŸ’¡ í•™ìŠµì„ ìœ„í•´ ìŠ¤ìœ„ì¹˜ í¬íŠ¸ í†µê³„ ë°ì´í„° ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ì‘ì—…
```python
# Interface names to filter
interface_names = [ 
    "GigabitEthernet1/0/12",
    "GigabitEthernet1/0/13",
    "GigabitEthernet1/0/14",
    "GigabitEthernet1/0/17",
    "GigabitEthernet1/0/18",
    "GigabitEthernet1/0/22", #ì´ë¶€ë¶„ì´ 192.168.0.17ë¡œ ì‚¬ìš©í•  ì¸í„°í˜ì´ìŠ¤ ì •ë³´
    "GigabitEthernet1/0/24",
    "GigabitEthernet1/0/4",
    "GigabitEthernet1/0/5",
    "GigabitEthernet1/0/8"
]

# Filtering the DataFrame to include only rows where 'field_value' matches any of the interface names
filtered_data_all_interfaces = df_switch_sensor_data[df_switch_sensor_data['field_value'].isin(interface_names)]
# Assuming df_switch_sensor_data is already created and has a 'time' column with datetime strings

# Convert 'time' column to datetime format
filtered_data_all_interfaces['time'] = pd.to_datetime(filtered_data_all_interfaces['time'])

# Format 'time' column to 'YYYY-MM-DD HH:MM' format
filtered_data_all_interfaces['time'] = filtered_data_all_interfaces['time'].dt.strftime('%Y-%m-%d %H:%M')

filtered_data_all_interfaces.head()  # Displaying the first few rows to verify the transformation
# 'output' ì»¬ëŸ¼ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ (ì˜¤ë¥˜ ë°œìƒ ì‹œ NaNìœ¼ë¡œ ëŒ€ì²´)
filtered_data_all_interfaces['output'] = pd.to_numeric(filtered_data_all_interfaces['output'], errors='coerce')

# ë°ì´í„° ì •ë ¬ ë° ê·¸ë£¹í™”
grouped = filtered_data_all_interfaces.sort_values('time').groupby(['traffic_type', 'field_value'])

# 'output' ê°’ì˜ ì‹œê°„ëŒ€ë³„ ì¦ê°€ë¶„ì„ ê³„ì‚°
filtered_data_all_interfaces['output_diff'] = grouped['output'].diff().fillna(0)

# 32ë¹„íŠ¸ ì´ˆê³¼ ìƒí™© ê°ì§€ ë° ì²˜ë¦¬
max_32bit_value = 2**32 - 1

def adjust_negative_diff(row):
    if row['output_diff'] < 0:
        return row['output_diff'] + max_32bit_value + 1
    return row['output_diff']

filtered_data_all_interfaces['output_diff'] = filtered_data_all_interfaces.apply(adjust_negative_diff, axis=1)

pivot_table = filtered_data_all_interfaces.pivot_table(
    index=['field_value', 'time'],
    columns='traffic_type',
    values='output_diff',
    aggfunc='sum'  
).reset_index()
```

## ğŸš€ ì‹¤í–‰ ë°©ë²•

```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ í•™ìŠµ ëª¨ë“œ ì‹¤í–‰(êµ¬í˜„ì¤‘)
$ python main.py --mode train

```

---

## ğŸ§  ëª¨ë¸ êµ¬ì„± ìš”ì•½

| êµ¬ì„± ìš”ì†Œ     | ê¸°ë²•                 | ì—­í•                              |
|---------------|----------------------|----------------------------------|
| ğŸ”¹ ì„¸ì…˜ ë¶„ì„  | VAE                  | ì •ìƒ/ë¹„ì •ìƒ ì„¸ì…˜ êµ¬ë¶„           |
| ğŸ”¹ ì‹œê³„ì—´ ë¶„ì„| Anomaly Transformer | ì„¸ì…˜ íë¦„ ê¸°ë°˜ ì´ìƒ íƒì§€       |
| ğŸ”¹ ë‹¨ë§ íŒë‹¨  | Naive Bayes          | ì´ìƒ í™•ë¥  ê¸°ë°˜ ë‹¨ë§ ìˆœìœ„í™”     |

---

## ğŸ›¡ï¸ ì ìš© ì‹œë‚˜ë¦¬ì˜¤

- âœ… ë‚´ë¶€ ì—…ë¬´ë§ì— ì™¸ë¶€ë§ í˜¼ìš© ì—¬ë¶€ ìë™ íƒì§€  
- âœ… ë¹„ì¸ê°€ NAT ê³µìœ ê¸°Â·í—ˆë¸Œ íƒì§€ ë° ê²½ê³   
- âœ… ì„œë²„ ì ‘ê·¼ íŒ¨í„´ ê¸°ë°˜ ì´ìƒ ë‹¨ë§ íƒì§€  
- âœ… í¬íŠ¸ë³„ íŠ¸ë˜í”½ ê¸‰ì¦, ì˜¤ë¥˜ íŒ¨í„´ íƒì§€

---
# ğŸ“Œ ì œì•ˆ ë°©ë²• ë° ëª¨ë¸ (Proposed Methods and Models)

ë³¸ í”„ë¡œì íŠ¸ëŠ” ì‹¤ì‹œê°„ ë‹¤ì°¨ì› ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ì´ìƒ íŠ¸ë˜í”½ì„ íƒì§€í•˜ê³  ì°¨ë‹¨í•˜ëŠ” **ê³„ì¸µì  íƒì§€ í”„ë ˆì„ì›Œí¬**ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.  
ë‹¤ìŒì€ ì œì•ˆ ëª¨ë¸ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì™€ ê°ê°ì˜ êµ¬í˜„ ë°©ë²•ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì…ë‹ˆë‹¤.

---

## ğŸ“– ëª©ì°¨

1. [ğŸ”¹ ì œì•ˆ ëª¨ë¸ ê°œìš”](#-ì œì•ˆ-ëª¨ë¸-ê°œìš”)
2. [ğŸ”¹ VAE ê¸°ë°˜ ì„¸ì…˜ ì´ìƒ íƒì§€](#-vae-ê¸°ë°˜-ì„¸ì…˜-ì´ìƒ-íƒì§€)
3. [ğŸ”¹ ì„œë²„ ì ‘ê·¼ íŒ¨í„´ ê¸°ë°˜ ì´ìƒ íƒì§€ (Poisson ë¶„í¬ í™œìš©)](#-ì„œë²„-ì ‘ê·¼-íŒ¨í„´-ê¸°ë°˜-ì´ìƒ-íƒì§€-poisson-ë¶„í¬-í™œìš©)
4. [ğŸ”¹ ì„¸ì…˜ íë¦„ ê¸°ë°˜ ì´ìƒ íƒì§€ (Anomaly Transformer)](#-ì„¸ì…˜-íë¦„-ê¸°ë°˜-ì´ìƒ-íƒì§€-anomaly-transformer)
5. [ğŸ”¹ ìŠ¤ìœ„ì¹˜ í¬íŠ¸ í†µê³„ ë°ì´í„° ê¸°ë°˜ ì´ìƒ íƒì§€](#-ìŠ¤ìœ„ì¹˜-í¬íŠ¸-í†µê³„-ë°ì´í„°-ê¸°ë°˜-ì´ìƒ-íƒì§€)
6. [ğŸ”¹ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆë¥¼ ì´ìš©í•œ ì´ìƒ ë‹¨ë§ í™•ë¥ ì  íŒë‹¨](#-ë‚˜ì´ë¸Œ-ë² ì´ì¦ˆë¥¼-ì´ìš©í•œ-ì´ìƒ-ë‹¨ë§-í™•ë¥ ì -íŒë‹¨)
7. [ğŸ”¹ ì•™ìƒë¸” ê¸°ë°˜ íƒì§€ ë° ìµœì¢… íŒë‹¨](#-ì•™ìƒë¸”-ê¸°ë°˜-íƒì§€-ë°-ìµœì¢…-íŒë‹¨)

---

## ğŸ”¹ ì œì•ˆ ëª¨ë¸ ê°œìš”

### ğŸ”¹ ì œì•ˆ ëª¨ë¸ì˜ íŠ¹ì§•ê³¼ ì¥ì 

ë³¸ í”„ë¡œì íŠ¸ì—ì„œ ì œì•ˆí•˜ëŠ” ì´ìƒ íŠ¸ë˜í”½ íƒì§€ ëª¨ë¸ì€ ë§ë¶„ë¦¬ í™˜ê²½ì˜ ë‹¤ì–‘í•œ ë³´ì•ˆ ìœ„í˜‘ì„ íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì‘í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë‹¨ì¼ ë°©ì‹ì˜ ì´ìƒ íƒì§€ ê¸°ë²•ì´ ê°–ëŠ” í•œê³„ë¥¼ ë³´ì™„í•˜ê³ , ë‹¤ì–‘í•œ íƒì§€ ëª¨ë¸ì„ ì•™ìƒë¸” í˜•íƒœë¡œ ê²°í•©í•˜ì—¬ ë†’ì€ ì •í™•ë„ì™€ ì‹ ì†í•œ íƒì§€ ëŠ¥ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ë³¸ ëª¨ë¸ì˜ ì£¼ìš” íŠ¹ì§•ê³¼ ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

* **ë‹¤ì°¨ì› ë°ì´í„° ê¸°ë°˜ íƒì§€**

  * ì„¸ì…˜ ë°ì´í„°, ì‹œê³„ì—´ ë°ì´í„°, í¬íŠ¸ í†µê³„ ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë™ì‹œì— í™œìš©í•˜ì—¬ íƒì§€ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.

* **ë¹„ì§€ë„ í•™ìŠµ ê¸°ë°˜ ì´ìƒ íƒì§€**

  * Variational Autoencoder(VAE)ì™€ Anomaly Transformerë¥¼ í†µí•´ ì •ìƒ íŒ¨í„´ë§Œì„ í•™ìŠµí•˜ì—¬ ë ˆì´ë¸”ë§ì´ ì–´ë ¤ìš´ í™˜ê²½ì—ì„œë„ ìš°ìˆ˜í•œ íƒì§€ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

* **ì‹¤ì‹œê°„ í™•ë¥ ì  íŒë‹¨ ë° ìš°ì„ ìˆœìœ„í™”**

  * ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ(Naive Bayes) ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ê° ë‹¨ë§ì˜ ì´ìƒ í–‰ìœ„ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ë¥ ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬, íƒì§€ëœ ìœ„í˜‘ì— ì‹ ì†í•œ ìš°ì„  ëŒ€ì‘ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

* **ì•™ìƒë¸” ê¸°ë²•ì„ í†µí•œ ê²¬ê³ í•œ íƒì§€**

  * ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ì ì¸ íƒì§€ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ í†µí•©(ì•™ìƒë¸”)í•˜ì—¬, ê°œë³„ ëª¨ë¸ì˜ ì˜¤íƒì§€(false positive) ë° ë¯¸íƒì§€(false negative)ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê°ì†Œì‹œí‚µë‹ˆë‹¤.

* **ìœ ì—°í•œ í™•ì¥ì„± ë° ë²”ìš©ì„±**

  * ì„œë¹„ìŠ¤ í™˜ê²½ì´ ë³€ê²½ë˜ê±°ë‚˜ ìƒˆë¡œìš´ íŠ¸ë˜í”½ íŒ¨í„´ì´ ë°œìƒí•´ë„, ê°œë³„ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì¡°ì •ì„ í†µí•´ ìœ ì—°í•˜ê²Œ ëŒ€ì‘ì´ ê°€ëŠ¥í•˜ë©° ë‹¤ì–‘í•œ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì—ì„œ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

### ğŸ”¹ ëª¨ë¸ êµ¬ì¡° ë° ë°ì´í„° íë¦„ë„

ë³¸ í”„ë¡œì íŠ¸ì˜ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œì€ **4ê°œì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ**ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©°, ê° êµ¬ì„± ìš”ì†ŒëŠ” ë…ë¦½ì ì´ë©´ì„œë„ ìƒí˜¸ë³´ì™„ì ì¸ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì „ì²´ í”„ë¡œì„¸ìŠ¤ëŠ” ì•„ë˜ì™€ ê°™ì€ ë‹¨ê³„ë¡œ ì§„í–‰ë©ë‹ˆë‹¤:

#### ğŸ–¥ï¸ ëª¨ë¸ êµ¬ì¡° ê°œìš”:

| êµ¬ì„± ìš”ì†Œ       | ì‚¬ìš© ëª¨ë¸                         | ì—­í•  ë° ê¸°ëŠ¥                      |
| ----------- | ----------------------------- | ---------------------------- |
| ì„¸ì…˜ ê¸°ë°˜ ì´ìƒ íƒì§€ | Variational Autoencoder (VAE) | ì„¸ì…˜ ë°ì´í„° ì¬êµ¬ì„± ì˜¤ì°¨ ê¸°ë°˜ ì´ìƒ í–‰ìœ„ íƒì§€    |
| ì„œë²„ ì ‘ê·¼ íŒ¨í„´ ë¶„ì„ | Poisson ë¶„í¬ ê¸°ë°˜ í†µê³„ ëª¨ë¸           | ì„œë²„ ì ‘ê·¼ íšŸìˆ˜ì— ë”°ë¥¸ ë¹„ì •ìƒì  ì ‘ê·¼ íŒ¨í„´ íƒì§€   |
| ì„¸ì…˜ íë¦„ ì´ìƒ íƒì§€ | Anomaly Transformer           | ì„¸ì…˜ íë¦„ì—ì„œ ë°œìƒí•˜ëŠ” ì‹œê³„ì—´ ì´ìƒ íƒì§€       |
| ì´ìƒ ë‹¨ë§ í™•ë¥  í‰ê°€ | Naive Bayes                   | ê°œë³„ ë‹¨ë§ ì´ìƒ í–‰ìœ„ í™•ë¥ ì  í‰ê°€ ë° ìœ„í—˜ë„ ìˆœìœ„í™” |

#### ğŸ“Œ ë°ì´í„° íë¦„ë„:

ì•„ë˜ íë¦„ì€ ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ì´ìƒ íƒì§€, ëŒ€ì‘ê¹Œì§€ì˜ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ê°„ëµíˆ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤:

```plaintext
[ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ ìˆ˜ì§‘]
        â”‚
        â–¼
[ì‹¤ì‹œê°„ ë°ì´í„° ì „ì²˜ë¦¬]
(ì„¸ì…˜ ë°ì´í„°, í¬íŠ¸ í†µê³„ ë°ì´í„° ì¶”ì¶œ)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        [ì´ìƒ íƒì§€ ì•™ìƒë¸”]                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚ â”‚    1. VAE (ì„¸ì…˜ ì´ìƒ íƒì§€)     â”‚       â”‚
â”‚ â”‚    2. Poisson (ì ‘ê·¼ íŒ¨í„´ ë¶„ì„) â”‚       â”‚
â”‚ â”‚    3. Transformer (ì„¸ì…˜ íë¦„)  â”‚       â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚              â”‚ ê²°ê³¼ í†µí•©(í™•ë¥  ê°€ì¤‘ì¹˜ì ìš©) â”‚
â”‚              â–¼                           â”‚
â”‚         [4. Naive Bayes]                 â”‚
â”‚ (ë‹¨ë§ ì´ìƒ í™•ë¥  ê³„ì‚° ë° ìˆœìœ„í™”)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
[ê´€ë¦¬ì ì•Œë¦¼ ë° ìë™ ëŒ€ì‘]
(ì´ìƒ ë‹¨ë§ ì°¨ë‹¨, ê´€ë¦¬ì ê²½ê³  ë“±)
```

**ğŸ“Œ Note(ê·¸ë¦¼4)**

ã€A Real-time Multidimensional Data-driven Probabilistic Network Anomaly Detection Frameworkã€

 ![image](https://github.com/user-attachments/assets/fd4fdaa4-6884-4052-a352-b013cf90ff2e)















---

## ğŸ”¹ VAE ê¸°ë°˜ ì„¸ì…˜ ì´ìƒ íƒì§€

- **VAE (Variational Autoencoder) ì†Œê°œ**
- **VAEì˜ ì‚¬ìš© ëª©ì ê³¼ ì´ìœ **
- **ë°ì´í„° ì „ì²˜ë¦¬ ë° ì ì¬ ë²¡í„°(Latent Vector) ë³€í™˜ ë°©ë²•**

**ì˜ˆì‹œ ì½”ë“œ ì‚½ì…**:
```python
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Reshape, BatchNormalization, Dropout
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ë° ì „ì²˜ë¦¬
data_scaling = merged_df.astype('float32')
data = data_scaling.apply(min_max_scaling)

class DenseBlock(Model):
    def __init__(self, units, dropout_rate=0.3):
        super(DenseBlock, self).__init__()
        self.dense = Dense(units, activation=tf.nn.relu)
        self.bn = BatchNormalization()
        self.dropout = Dropout(dropout_rate)

    def call(self, x):
        x = self.dense(x)
        x = self.bn(x)
        return self.dropout(x)

class Encoder(Model):
    def __init__(self, latent_dim=10):
        super(Encoder, self).__init__()
        self.latent_dim = latent_dim
        self.dense_block1 = DenseBlock(256, dropout_rate=0.3)
        self.dense_block2 = DenseBlock(128, dropout_rate=0.3)
        self.dense_block3 = DenseBlock(64, dropout_rate=0.3)  # ì¶”ê°€ ê³„ì¸µ
        self.dense_output = Dense(units=self.latent_dim, activation=None)

    def call(self, x):
        x = self.dense_block1(x)
        x = self.dense_block2(x)
        x = self.dense_block3(x)  # ì¶”ê°€ ê³„ì¸µ í˜¸ì¶œ
        return self.dense_output(x)

class Decoder(Model):
    def __init__(self, latent_dim=10):
        super(Decoder, self).__init__()
        self.dense_block1 = DenseBlock(64, dropout_rate=0.3)
        self.dense_block2 = DenseBlock(128, dropout_rate=0.3)
        self.dense_block3 = DenseBlock(256, dropout_rate=0.3)  # ì¶”ê°€ ê³„ì¸µ
        self.dense_output = Dense(units=86, activation='sigmoid')  # ë°ì´í„°ì— ë”°ë¼ ì¡°ì •

    def call(self, x):
        x = self.dense_block1(x)
        x = self.dense_block2(x)
        x = self.dense_block3(x)  # ì¶”ê°€ ê³„ì¸µ í˜¸ì¶œ
        return self.dense_output(x)

class VAE(Model):
    def __init__(self, latent_dim=10):
        super(VAE, self).__init__()
        self.latent_dim = latent_dim
        self.encoder = Encoder(latent_dim=self.latent_dim)
        self.decoder = Decoder(latent_dim=self.latent_dim)
        self.dense_mu = Dense(units=self.latent_dim, activation=None)
        self.dense_log_var = Dense(units=self.latent_dim, activation=None)

    def reparameterize(self, mu, logvar):
        eps = tf.random.normal(shape=mu.shape)
        return eps * tf.exp(logvar * 0.5) + mu

    def call(self, x):
        x = self.encoder(x)
        mu = self.dense_mu(x)
        log_var = self.dense_log_var(x)
        z = self.reparameterize(mu, log_var)
        x_recon = self.decoder(z)
        return x_recon, mu, log_var
    
    def encode(self, x):
        x = self.encoder(x)
        return self.dense_mu(x)

# ì†ì‹¤ í•¨ìˆ˜ ë° í•™ìŠµ ë‹¨ê³„ ì •ì˜
def compute_loss(data, reconstruction, mu, log_var):
    epsilon = 1e-7  # Small constant for numerical stability
    recon_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.mean_squared_error(data, reconstruction), axis=-1))
    kl_loss = -0.5 * (1 + log_var - tf.square(mu) - tf.exp(log_var + epsilon))
    kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))
    total_loss = recon_loss + kl_loss
    return total_loss, recon_loss, kl_loss

@tf.function
def train_step(model, optimizer, data):
    with tf.GradientTape() as tape:
        reconstruction, mu, log_var = model(data)
        data = tf.reshape(data, shape=(data.shape[0], -1))  # Ensuring shapes match
        reconstruction = tf.reshape(reconstruction, shape=(reconstruction.shape[0], -1))
        total_loss, recon_loss, kl_loss = compute_loss(data, reconstruction, mu, log_var)
    gradients = tape.gradient(total_loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return total_loss, recon_loss, kl_loss

# ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
latent_dim = 4  # ì ì¬ ì°¨ì› í¬ê¸° ì¡°ì • ê°€ëŠ¥
vae = VAE(latent_dim=latent_dim)

# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
initial_learning_rate = 1e-4
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)

# ë°ì´í„°ì…‹ ì¤€ë¹„ ë° í•™ìŠµ ë£¨í”„
dataset = tf.data.Dataset.from_tensor_slices(data.values).shuffle(len(data)).batch(64)

# í•™ìŠµ ë£¨í”„
num_epochs = 100  # ì¡°ì • ê°€ëŠ¥
for epoch in range(num_epochs):
    for step, batch_data in enumerate(dataset):
        total_loss, recon_loss, kl_loss = train_step(vae, optimizer, batch_data)
    print(f"Epoch: {epoch+1}, Total Loss: {total_loss.numpy()}, Recon Loss: {recon_loss.numpy()}, KL Loss: {kl_loss.numpy()}")

# ì ì¬ ê³µê°„ ì‹œê°í™”
def visualize_latent_space(model, data, num_samples=1000):
    subset = data.sample(n=num_samples, random_state=42)  # ë¬´ì‘ìœ„ ìƒ˜í”Œ ì„ íƒ

    mu = model.encode(subset).numpy()
    
    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
    mu_tsne = tsne.fit_transform(mu)

    plt.figure(figsize=(10, 8))
    plt.scatter(mu_tsne[:, 0], mu_tsne[:, 1], alpha=0.5)
    plt.title('Latent Space Representation using t-SNE')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.show()

# visualize_latent_space(vae, data)
```

---

## ğŸ”¹ ì„œë²„ ì ‘ê·¼ íŒ¨í„´ ê¸°ë°˜ ì´ìƒ íƒì§€ (Poisson ë¶„í¬ í™œìš©)

- **Poisson ë¶„í¬ ê°œë…ê³¼ ë„ì… ì´ìœ **
- **ì ‘ê·¼ ë¹ˆë„ ì´ìƒ íƒì§€ ë°©ë²•**
- **Î»(ëŒë‹¤) ê°’ ì‚°ì¶œ ë° ë¶„ì„ ë°©ë²•**

**ì˜ˆì‹œ ì½”ë“œ ì‚½ì…**:
```python
import pandas as pd
import numpy as np
from scipy.stats import poisson

# ì˜ˆì‹œ ë°ì´í„° ìƒì„±
# data = read_csv(...)

# ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜ ë° ì „ì²˜ë¦¬
df = pd.DataFrame(data)
df['@timestamp'] = pd.to_datetime(df['@timestamp'], unit='ms')
df['source_ip'] = df['source'].apply(lambda x: x['ip'])
df['destination_ip'] = df['destination'].apply(lambda x: x['ip'])
df = df.drop(columns=['source', 'destination'])
df['hour'] = df['@timestamp'].dt.hour

# ì‹œê°„ëŒ€ë³„ ë°©ë¬¸ ë¹ˆë„ ê³„ì‚°
visits_per_hour = df.groupby(['source_ip', 'destination_ip', 'hour']).size().reset_index(name='visits')

# ì‹œê°„ëŒ€ë³„ í‰ê·  ë°©ë¬¸ ë¹ˆë„ ê³„ì‚°
average_visits_per_hour = visits_per_hour.groupby(['source_ip', 'destination_ip', 'hour'])['visits'].mean().reset_index(name='average_visits')

# í‘¸ì•„ì†¡ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒìœ„ 95% ì„ê³„ì¹˜ ê³„ì‚° í•¨ìˆ˜
def calculate_threshold(avg_visits):
    return poisson.ppf(0.99, avg_visits)

# ì‹œê°„ëŒ€ë³„ ìƒìœ„ 99% ì„ê³„ì¹˜ ê³„ì‚°
average_visits_per_hour['threshold'] = average_visits_per_hour['average_visits'].apply(calculate_threshold)

```

---

## ğŸ”¹ ì„¸ì…˜ íë¦„ ê¸°ë°˜ ì´ìƒ íƒì§€ (Anomaly Transformer)

- **Anomaly Transformer ëª¨ë¸ ì†Œê°œ**
- **Association Discrepancy ê°œë… ì„¤ëª…**
- **ì„¸ì…˜ íë¦„ ë°ì´í„°ì˜ ëª¨ë¸ í•™ìŠµ ë° ì´ìƒ íƒì§€ ê³¼ì •**


Anomaly-Transformer (ICLR 2022 Spotlight)

https://github.com/thuml/Anomaly-Transformer


---

## ğŸ”¹ ìŠ¤ìœ„ì¹˜ í¬íŠ¸ í†µê³„ ë°ì´í„° ê¸°ë°˜ ì´ìƒ íƒì§€ (Anomaly Transformer)

- **ìŠ¤ìœ„ì¹˜ í¬íŠ¸ í†µê³„ ë°ì´í„°ì˜ ì¤‘ìš”ì„±**
- **ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ ë°©ë²•**
- **ì´ìƒ ì§•í›„ íŒë‹¨ ê¸°ì¤€ ë° íƒì§€ ë°©ì‹**

Anomaly-Transformer (ICLR 2022 Spotlight)

https://github.com/thuml/Anomaly-Transformer

---

## ğŸ”¹ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆë¥¼ ì´ìš©í•œ ì´ìƒ ë‹¨ë§ í™•ë¥ ì  íŒë‹¨

- **ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ ì†Œê°œ ë° ì„ íƒ ì´ìœ **
- **ë‹¨ë§ ì´ìƒ í™•ë¥  ê³„ì‚° ë°©ë²• ë° ê³µì‹**
- **ë¡œê·¸ í™•ë¥  ë³€í™˜ ë° ì†Œí”„íŠ¸ë§¥ìŠ¤ í™œìš© ì„¤ëª…**

**ì˜ˆì‹œ ì½”ë“œ ì‚½ì…**:
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.special import softmax

# ê°€ì¤‘ì¹˜(ê°ë§ˆ) ì„¤ì •
gamma_poisson_vae = 0.5  # VAEì™€ POISSONì— ì ìš©í•  ê°€ì¤‘ì¹˜
gamma_sw_vae_mus = 1.0   # SWì™€ VAE_MUSì— ì ìš©í•  ê°€ì¤‘ì¹˜

# íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
vae_df = pd.read_csv('NaiveBayesResults/VAE.csv', index_col=0)  # VAE.csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
poisson_df = pd.read_csv('NaiveBayesResults/POISSON.csv', index_col=0)  # POISSON.csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
sw_df = pd.read_csv('NaiveBayesResults/SW.csv', index_col=0)  # SW.csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
vae_mus_df = pd.read_csv('NaiveBayesResults/VAE_MUS.csv', index_col=0)  # VAE_MUS.csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

# ê° ë°ì´í„°í”„ë ˆì„ì˜ í¬ê¸° í™•ì¸ (ë””ë²„ê¹…ìš© ì¶œë ¥)
print("VAE shape:", vae_df.shape)
print("POISSON shape:", poisson_df.shape)
print("SW shape:", sw_df.shape)
print("VAE_MUS shape:", vae_mus_df.shape)

# ê° ë°ì´í„°í”„ë ˆì„ì˜ ì¸ë±ìŠ¤ (IP ì£¼ì†Œ) í™•ì¸ (ë””ë²„ê¹…ìš© ì¶œë ¥)
print("VAE IPs:", vae_df.index)
print("POISSON IPs:", poisson_df.index)
print("SW IPs:", sw_df.index)
print("VAE_MUS IPs:", vae_mus_df.index)

# IP ì£¼ì†Œ ì¤‘ë³µ ë˜ëŠ” ëˆ„ë½ í™•ì¸ì„ ìœ„í•œ í†µí•© ì¸ë±ìŠ¤ ë¹„êµ
common_ips = vae_df.index.intersection(poisson_df.index).intersection(sw_df.index).intersection(vae_mus_df.index)

# ë§Œì•½ ê³µí†µ IP ê°œìˆ˜ê°€ ì˜ˆìƒë³´ë‹¤ ì ì„ ê²½ìš° (9ê°œì¸ ê²½ìš°), ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ê²°ì¸¡ê°’ ì²˜ë¦¬ ìˆ˜í–‰
# ì´ ê²½ìš° ê° ë°ì´í„°í”„ë ˆì„ì„ ê³µí†µ IPë¡œ ë§ì¶”ì§€ ì•Šê³ , ëª¨ë“  IPë¥¼ í¬í•¨í•˜ë„ë¡ ê²°ì¸¡ê°’ì„ ì±„ìš°ëŠ” ë°©ë²•ì„ ì‚¬ìš©
combined_ips = vae_df.index.union(poisson_df.index).union(sw_df.index).union(vae_mus_df.index)

# ê° ë°ì´í„°í”„ë ˆì„ì˜ ê²°ì¸¡ê°’ì„ 0ìœ¼ë¡œ ì±„ì›Œì„œ ëª¨ë“  IPë¥¼ í¬í•¨í•˜ë„ë¡ ì²˜ë¦¬
vae_df_numeric = vae_df.reindex(combined_ips).fillna(0)
poisson_df_numeric = poisson_df.reindex(combined_ips).fillna(0)
sw_df_numeric = sw_df.reindex(combined_ips).fillna(0)
vae_mus_df_numeric = vae_mus_df.reindex(combined_ips).fillna(0)

# ë°ì´í„° ê°€ì¤‘ì¹˜ ì ìš©
vae_df_weighted = vae_df_numeric * gamma_poisson_vae  # VAE ë°ì´í„°ì— ê°€ì¤‘ì¹˜ ì ìš©
poisson_df_weighted = poisson_df_numeric * gamma_poisson_vae  # POISSON ë°ì´í„°ì— ê°€ì¤‘ì¹˜ ì ìš©
sw_df_weighted = sw_df_numeric * gamma_sw_vae_mus  # SW ë°ì´í„°ì— ê°€ì¤‘ì¹˜ ì ìš©
vae_mus_df_weighted = vae_mus_df_numeric * gamma_sw_vae_mus  # VAE_MUS ë°ì´í„°ì— ê°€ì¤‘ì¹˜ ì ìš©

# ë„¤ ê°œì˜ íŒŒì¼ì„ í•©ì‚°í•˜ì—¬ í•˜ë‚˜ì˜ DataFrame ìƒì„± (ìˆ«ì ë°ì´í„°ë§Œ)
combined_df_numeric = vae_df_weighted + poisson_df_weighted + sw_df_weighted + vae_mus_df_weighted

# ë°ì´í„°ì˜ í¬ê¸° í™•ì¸ (ë””ë²„ê¹…ìš© ì¶œë ¥)
print("Combined data shape:", combined_df_numeric.shape)  # (10, 200)ì¸ì§€ í™•ì¸

# Softmax ë° Z-score ì •ê·œí™”ë¥¼ ì‚¬ìš©í•œ í™•ë¥  ê³„ì‚° í•¨ìˆ˜
def calculate_initial_probabilities(data):
    if np.all(data == data[0]):  # ë§Œì•½ ëª¨ë“  ê°’ì´ ë™ì¼í•˜ë‹¤ë©´
        return np.ones_like(data) / len(data)  # ë™ì¼í•œ í™•ë¥  ë¶„í¬
    normalized_scores = (data - np.mean(data)) / (np.std(data) + 1e-10)  # Z-score ì •ê·œí™”
    probabilities = softmax(normalized_scores)  # softmaxë¡œ í™•ë¥  ê³„ì‚°
    return probabilities

# ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ (ë¡œê·¸ ê³µê°„ì—ì„œ ê³„ì‚°)
def bayesian_update_log(prior_log, likelihood):
    likelihood = np.clip(likelihood, 1e-10, 1.0)  # í™•ë¥  ë²”ìœ„ë¥¼ 1e-10ë¡œ ì œí•œí•˜ì—¬ log(0) ë°©ì§€
    posterior_log = prior_log + np.log(likelihood)  # ë¡œê·¸ í™•ë¥ ì„ ë”í•˜ì—¬ posterior ê³„ì‚°
    posterior_log -= np.max(posterior_log)  # overflow ë°©ì§€ë¥¼ ìœ„í•´ ìµœëŒ€ê°’ì„ ë¹¼ì¤Œ
    posterior = np.exp(posterior_log)  # posterior ê°’ì„ í™•ë¥ ë¡œ ë³€í™˜
    return posterior / np.sum(posterior)  # ì •ê·œí™”

# ì‹œê°„ëŒ€ë³„ë¡œ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
def update_probabilities_over_time(data_df):
    probability_history = []  # í™•ë¥  ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    
    # ì²« ë²ˆì§¸ ì‹œê°„ëŒ€ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ˆê¸° í™•ë¥  ê³„ì‚°
    initial_data = data_df.iloc[0]
    initial_probabilities = calculate_initial_probabilities(initial_data)
    
    # ì´ˆê¸° í™•ë¥ ì„ log-spaceë¡œ ë³€í™˜
    prior_log = np.log(initial_probabilities + 1e-10)

    # ê° ì‹œê°„ëŒ€ë³„ë¡œ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì—…ë°ì´íŠ¸ ìˆ˜í–‰
    for t in range(1, len(data_df)):
        new_data = data_df.iloc[t]  # ìƒˆë¡œìš´ ë°ì´í„° ì¶”ì¶œ
        likelihood = calculate_initial_probabilities(new_data)  # ìƒˆë¡œìš´ ë°ì´í„°ì˜ ê°€ëŠ¥ë„ ê³„ì‚°
        prior_log = bayesian_update_log(prior_log, likelihood)  # ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì—…ë°ì´íŠ¸
        probability_history.append(prior_log)  # ì—…ë°ì´íŠ¸ëœ í™•ë¥  ê¸°ë¡
    
    return np.array(probability_history)  # í™•ë¥  ê¸°ë¡ ë°˜í™˜

# ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì—…ë°ì´íŠ¸ ì‹¤í–‰ (ì‹œê°„ëŒ€ë³„ë¡œ ê³„ì‚°)
updated_probabilities = update_probabilities_over_time(combined_df_numeric.T)

# í™•ë¥  ì—…ë°ì´íŠ¸ íˆìŠ¤í† ë¦¬ë¥¼ ì „ì¹˜í•˜ì—¬ IPë³„ë¡œ í–‰, ì‹œê°„ë³„ë¡œ ì—´ì´ ë˜ë„ë¡ ë³€í™˜
updated_probabilities = updated_probabilities.T

# ë°ì´í„°ì˜ í¬ê¸° í™•ì¸ (ë””ë²„ê¹…ìš© ì¶œë ¥)
print("Updated probabilities shape:", updated_probabilities.shape)  # (10, 200)ì¸ì§€ í™•ì¸

# ì‹œê°í™” ì„¤ì •
plt.figure(figsize=(60, 10))  # ê·¸ë˜í”„ í¬ê¸° ì„¤ì •
sns.set(font_scale=0.7)  # ê¸€ê¼´ í¬ê¸° ì¡°ì •

# xticklabelsì˜ ë’¤ ë‘ ìë¦¬ë§Œ í‘œì‹œ (00 ~ 99ë¡œ ë°˜ë³µ)
xticklabels = [f'{i % 100}' for i in range(updated_probabilities.shape[1])]

# íˆíŠ¸ë§µ ì‹œê°í™”
ax = sns.heatmap(updated_probabilities, xticklabels=xticklabels, 
                 yticklabels=combined_ips, cmap='YlOrRd', vmin=0, vmax=1, 
                 cbar_kws={'label': 'Probability', 'shrink': 0.4}, 
                 linewidths=0.01, linecolor='black', square=True, annot=True, fmt=".2f", annot_kws={"size": 6})

plt.title('Naive Bayesian Update of Probabilities over Time')
plt.xlabel('Time')
plt.ylabel('IP Address')
plt.tight_layout()

plt.show()

```
---


![image](https://github.com/user-attachments/assets/8a40318e-fd02-49b3-8957-2757f669feaf)

---

## ğŸ”¹ ì•™ìƒë¸” ê¸°ë°˜ íƒì§€ ë° ìµœì¢… íŒë‹¨

- **ì•™ìƒë¸” ê¸°ë²•ì˜ í•„ìš”ì„± ë° ê¸°ëŒ€ íš¨ê³¼**
- **ê° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì ìš© ë°©ë²• (Gamma ê°’ í™œìš©)**
- **ìµœì¢… ëª¨ë¸ì˜ íƒì§€ ì„±ëŠ¥ í‰ê°€ ë° í™œìš© ê°€ëŠ¥ì„±**

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ ë° ê´€ë ¨ ìë£Œ

- Xu, Jiehui et al. (2022). *Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy*
